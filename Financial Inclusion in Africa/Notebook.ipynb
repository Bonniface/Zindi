{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "Unable to start Kernel 'base (Python 3.9.7)' due to connection timeout. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# dataframe and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv('Train.csv', index_col=0, na_values='na')\n",
    "print(f'The DataFrame has {len(Train)} rows and {Train.shape[1]} columns.')\n",
    "SampleSubmission = pd.read_csv('SampleSubmission.csv', index_col=0, na_values='na')\n",
    "print(f'The DataFrame has {len(SampleSubmission)} rows and {SampleSubmission.shape[1]} columns.')\n",
    "Test = pd.read_csv('Test.csv', index_col=0, na_values='na')\n",
    "print(f'The DataFrame has {len(Test)} rows and {Test.shape[1]} columns.')\n",
    "Variables = pd.read_csv('VariableDefinition.csv', index_col=0, na_values='na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the columns for different datatypes:\n",
    "print('List of all Columns: ')\n",
    "print(Train.columns)\n",
    "print('\\n')\n",
    "print('Integer Type: ')\n",
    "Col_int = Train.select_dtypes(np.int64).columns\n",
    "print(Col_int)\n",
    "print('\\n')\n",
    "print('Float Type: ')\n",
    "Col_float = Train.select_dtypes(np.float64).columns\n",
    "print(Col_float)\n",
    "print('\\n')\n",
    "print('Object Type: ')\n",
    "Col_cat = Train.select_dtypes(object).columns\n",
    "print(Col_cat)\n",
    "print('\\n')\n",
    "print('Count:')\n",
    "print(Train.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING Correllation matrix\n",
    "corr_mat= Train.corr(method='pearson')\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import preprocessing module\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Cobvert target label to numerical Data\n",
    "le = LabelEncoder()\n",
    "train['bank_account'] = le.fit_transform(train['bank_account'])\n",
    "\n",
    "#Separate training features from target\n",
    "X_train = train.drop(['bank_account'], axis=1)\n",
    "y_train = train['bank_account']\n",
    "\n",
    "y_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess our data from train models\n",
    "def preprocessing_data(data):\n",
    "\n",
    "    # Convert the following numerical labels from interger to float\n",
    "    float_array = data[[\"household_size\", \"age_of_respondent\", \"year\"]].values.astype(float)\n",
    "    \n",
    "    # categorical features to be onverted to One Hot Encoding\n",
    "    categ = [\"relationship_with_head\",\n",
    "             \"marital_status\",\n",
    "             \"education_level\",\n",
    "             \"job_type\",\n",
    "             \"country\"]\n",
    "    \n",
    "    # One Hot Encoding conversion\n",
    "    data = pd.get_dummies(data, prefix_sep=\"_\", columns=categ)\n",
    "    \n",
    "    # Label Encoder conversion\n",
    "    data[\"location_type\"] = le.fit_transform(data[\"location_type\"])\n",
    "    data[\"cellphone_access\"] = le.fit_transform(data[\"cellphone_access\"])\n",
    "    data[\"gender_of_respondent\"] = le.fit_transform(data[\"gender_of_respondent\"])\n",
    "    \n",
    "    # drop uniquid column\n",
    "    data = data.drop([\"uniqueid\"], axis=1)\n",
    "    \n",
    "    # scale our data into range of 0 and 1\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data = scaler.fit_transform(data)\n",
    "    \n",
    "    return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the train data \n",
    "processed_train = preprocessing_data(X_train)\n",
    "processed_test = preprocessing_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first train row\n",
    "print(processed_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_Train, X_Val, y_Train, y_val = train_test_split(processed_train, y_train, stratify = y_train, \n",
    "                                                  test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import best_model\n",
    "best_model, best_model_name, acc = best_model.bestClassificationModel(X, y)\n",
    "\n",
    "print(best_model)\n",
    "Train.describe().transpose()\n",
    "print(best_model_name, \":\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def performance_evaluation_report(model, X_cv, y_cv, show_plot=False, labels=None, show_pr_curve=False):\n",
    "    '''\n",
    "    Function for creating a performance report of a classification model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : scikit-learn estimator\n",
    "        A fitted estimator for classification problems.\n",
    "    X_cv : pd.DataFrame\n",
    "        DataFrame with features matching y_test\n",
    "    y_cv : array/pd.Series\n",
    "        Target of a classification problem.\n",
    "    show_plot : bool\n",
    "        Flag whether to show the plot\n",
    "    labels : list\n",
    "        List with the class names.\n",
    "    show_pr_curve : bool\n",
    "        Flag whether to also show the PR-curve. For this to take effect, \n",
    "        show_plot must be True.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    stats : pd.Series\n",
    "        A series with the most important evaluation metrics\n",
    "    '''\n",
    "\n",
    "    y_pred = model.predict(X_cv)\n",
    "    y_pred_prob = model.predict_proba(X_cv)[:, 1]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_cv, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_cv, y_pred_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "        y_cv, y_pred_prob)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    if show_plot:\n",
    "\n",
    "        if labels is None:\n",
    "            labels = ['Negative', 'Positive']\n",
    "\n",
    "        N_SUBPLOTS = 3 if show_pr_curve else 2\n",
    "        PLOT_WIDTH = 15 if show_pr_curve else 12\n",
    "        PLOT_HEIGHT = 5 if show_pr_curve else 6\n",
    "\n",
    "        fig, ax = plt.subplots(\n",
    "            1, N_SUBPLOTS, figsize=(PLOT_WIDTH, PLOT_HEIGHT))\n",
    "        fig.suptitle('Performance Evaluation', fontsize=16)\n",
    "\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5, cmap='BuGn_r', square=True, cbar=False, ax=ax[0],\n",
    "                    annot_kws={\"ha\": 'center', \"va\": 'center'})\n",
    "        ax[0].set(xlabel='Predicted label',\n",
    "                  ylabel='Actual label', title='Confusion Matrix')\n",
    "        ax[0].xaxis.set_ticklabels(labels)\n",
    "        ax[0].yaxis.set_ticklabels(labels)\n",
    "\n",
    "        ax[1].plot(fpr, tpr, 'b-', label=f'ROC-AUC = {roc_auc:.2f}')\n",
    "        ax[1].set(xlabel='False Positive Rate',\n",
    "                  ylabel='True Positive Rate', title='ROC Curve')\n",
    "        ax[1].plot(fp/(fp+tn), tp/(tp+fn), 'ro',\n",
    "                   markersize=8, label='Decision Point')\n",
    "        ax[1].plot([0, 1], [0, 1], 'r--')\n",
    "        ax[1].legend(loc='lower right')\n",
    "\n",
    "        if show_pr_curve:\n",
    "\n",
    "            ax[2].plot(recall, precision, label=f'PR-AUC = {pr_auc:.2f}')\n",
    "            ax[2].set(xlabel='Recall', ylabel='Precision',\n",
    "                      title='Precision-Recall Curve')\n",
    "            ax[2].legend()\n",
    "\n",
    "#         print('#######################')\n",
    "#         print('Evaluation metrics ####')\n",
    "#         print('#######################')\n",
    "#         print(f'Accuracy: {metrics.accuracy_score(y_cv, y_pred):.4f}')\n",
    "#         print(f'Precision: {metrics.precision_score(y_cv, y_pred):.4f}')\n",
    "#         print(f'Recall (Sensitivity): {metrics.recall_score(y_cv, y_pred):.4f}')\n",
    "#         print(f'Specificity: {(tn / (tn + fp)):.4f}')\n",
    "#         print(f'F1-Score: {metrics.f1_score(y_cv, y_pred):.4f}')\n",
    "#         print(f\"Cohen's Kappa: {metrics.cohen_kappa_score(y_cv, y_pred):.4f}\")\n",
    "\n",
    "    stats = {'accuracy': metrics.accuracy_score(y_cv, y_pred),\n",
    "             'precision': metrics.precision_score(y_cv, y_pred),\n",
    "             'recall': metrics.recall_score(y_cv, y_pred),\n",
    "             'specificity': (tn / (tn + fp)),\n",
    "             'f1_score': metrics.f1_score(y_cv, y_pred),\n",
    "             'cohens_kappa': metrics.cohen_kappa_score(y_cv, y_pred),\n",
    "             'roc_auc': roc_auc,\n",
    "             'pr_auc': pr_auc}\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_cv)\n",
    "\n",
    "\n",
    "LABELS = ['No Cliam', 'Cliam']\n",
    "tree_perf = performance_evaluation_report(classifier, \n",
    "                                         X_cv, \n",
    "                                         y_cv, labels=LABELS, \n",
    "                                         show_plot=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)] # Number of estimators\n",
    "max_features = ['auto' , 'sqrt'] # Number of feature to consider at every split\n",
    "max_depth = [2 , 4] # Maximum number of level in tree\n",
    "min_sample_split = [2 , 5] # Minimum number of samples required to split a node\n",
    "min_sample_leaf = [1 , 2] # Minimum number of sample required at each leaf node\n",
    "bootstrap = [True , False] # Method of selecting sample for training each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Param_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_sample_split,\n",
    "    'min_samples_leaf': min_sample_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Optimize model parameters\n",
    "# I run this code in google colab to make the execution much faster and use the best params in the next code\n",
    "param_grid = {'min_child_weighth': [1, 5, 10],\n",
    "        'gamma': [0.5, 1],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 5]\n",
    "        }\n",
    "my_xgb_model = GridSearchCV(xg_model, param_grid,n_jobs=-1,verbose=2,cv=5)\n",
    "my_xgb_model.fit(X_Train, y_Train)\n",
    "print(my_xgb_model.best_params_) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02ac588134a123d94ce0e5d46d6e0eac0ad6d4a40dbbe595578d903144df55c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
